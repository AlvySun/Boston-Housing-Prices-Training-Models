from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
import numpy as np 
import pandas as pd 
import hvplot.pandas
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline
%config InlineBackend.figure_format = 'retina'
plt.style.use("fivethirtyeight")
pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) 

boston = datasets.load_boston()
X = boston.data 
y = boston.target 
df = pd.DataFrame(X, columns = boston.feature_names)
df.head()

# Data statistics are primarily for numeric fields.In this case, the data are just numeric fields, 
# and we can quickly see each field: median, variance, minimum, quarter of a digit, etc.
df["MEDV"] = y
df.head()

corr = df.corr()
plt.figure(figsize=(16,10))
sns.heatmap(corr, annot=True, fmt=".2f", cmap = "hot_r")
plt.show()

# View the correlation coefficient between each feature and the target variable MEDV.
corr["MEDV"].sort_values()

# Study the relationship between different independent variables, independent variables and dependent variables.
sns.pairplot(df[["LSTAT","INDUS","PTRATIO","MEDV"]]) 
plt.show()

# Python code that splits the original Boston housing dataset into two subsets: training/validation (80%), and test (20%).
X = df.drop("MEDV",axis=1)
y = df[["MEDV"]]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)
print("X_train shape: ", X_train.shape)
print("y_train shape: ", y_train.shape)
print("X_test shape: ", X_test.shape)
print("y_test shape: ", y_test.shape)

# Linear Regression
le = LinearRegression()
le.fit(X_train, y_train)
coef1 = le.coef_
coef1

# Prediction of test set data.
predict1 = le.predict(X_test)  
predict1[:5]

# The main objectives were to examine the scores of two indicators:
## • score score on test set
## • RM between test data and forecast dataSE Score
print("Score：", le.score(X_test, y_test))
print("RSME：", np.sqrt(mean_squared_error(y_test, predict1)))

# The following are the 13 regression coefficients obtained from this modeling case.
coef1

le_df = pd.DataFrame()
le_df["name"] = X.columns.tolist()
le_df["coef"] = coef1.reshape(-1,1)
le_df

# Compare the real value and predictive value.
test_pre = pd.DataFrame({"test": y_test["MEDV"].tolist(),"pre": predict1.flatten()})
test_pre

test_pre.plot(figsize=(18,10))
plt.show()

# When we compare the true value with the predicted value, 
# we find that 42.15% of the test sets have true values greater than predicted values
len(test_pre.query("test > pre"))/len(test_pre)

# Plot the scatter distribution of true and predicted values on the coordinate axis.
plt.scatter(y_test, predict1, label="test")
plt.plot([y_test.min(), y_test.max()],[y_test.min(), y_test.max()],'k--',lw=3,label="predict")
plt.show()

# As we can see from the picture above:
# Between 10-30 are a more accurate forecasting of house prices.
# After 30, the prediction value will be bigger than the true value.

# Whole data set evaluation
predict_all = le.predict(X)
print("Score：", le.score(X, y))
print("RSME：", np.sqrt(mean_squared_error(y, predict_all)))

# Compare true and predicted values on the overall data set.
all_pre = pd.DataFrame({"test": y["MEDV"].tolist(),"pre": predict_all.flatten()})
all_pre

plt.scatter(y, predict_all, label="y_all")
plt.plot([y.min(), y.max()],[y.min(), y.max()],'k--',lw=3,label="all_predict")
plt.show()

